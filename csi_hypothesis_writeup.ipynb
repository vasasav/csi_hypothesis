{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcaf1de-9d47-4831-989f-bce4bb7e345d",
   "metadata": {},
   "source": [
    "# Characteristic Stability Index (CSI) as a statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c7cc-aa07-42cb-a15c-aa6e38ef7937",
   "metadata": {},
   "source": [
    "<img src=\"thumbpic.png\" alt=\"UK with cities of interest marked\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff96cc-b762-47f8-97c1-0cde99510c35",
   "metadata": {},
   "source": [
    "Characteristic Stability Index (CSI), and the closely related Population Stability Index (PSI) are amongest recognized metrics to measure the distribution stability for numeric features.  Commonly CSI is presented as a universal metric with rule-of-thumb thresholds that are understood to have wide applicability. In this post I will instead treat it as a statistic and demonstrate that sample size and the underlying randomness of the data source can drastically alter what one should count as large or small for CSI. I will also present a JAX-based Python library to estimate appropriate CSI thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ea03c-1194-4d8e-b26d-2cce86f14323",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9a22a-96d7-4918-bfb8-f1b344fde183",
   "metadata": {},
   "source": [
    "For simplicity, we shall focus on a real-valued feature such as temperature *X*. Let there be the reference set of observations of *X*, which are recognized as 'normal', call it the 'left' set, and a new set of observations which need to be compared to normal, call that the 'right' set:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X^{(L)}&\\to {X^{(L)}_1,\\dots X^{(L)}_N},\\quad left\\,set \\\\\n",
    "X^{(R)}&\\to {X^{(R)}_1,\\dots X^{(R)}_M},\\quad right\\,set \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One then bins the observations into, typically, 10 bins such that each bin contains 10% of the observations from the left set:\n",
    "\n",
    "| bin number | bin lower bound | bin upper bound | number of left-set observations in bin                    | number of right-set observations in bin                    |\n",
    "|------------|-----------------|-----------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
    "| 0          | $B_0$           | $B_1$           | $A^{(L)}_0 \\approx N/10$                                  | $A^{(R)}_0$                                                |\n",
    "| 1          | $B_1$           | $B_2$           | $A^{(L)}_1 \\approx N/10$                                  | $A^{(R)}_1$                                                |\n",
    "| ...        | ...             | ...             | ...                                                       | ...                                                        |\n",
    "| 9          | $B_9$           | $B_{10}$        | $A^{(L)}_9 \\approx N/10$                                  | $A^{(R)}_9$                                                |\n",
    "\n",
    "**Note:** Throughout the text I shall refer to arrays of occupancy, the fourth and fifth columns in the table above as *histograms* to simplify text. \n",
    "\n",
    "The CSI computed from the relative occupancy of each bin, i.e. for $\\rho^{(L)}_i=A^{(L)}_i/N$ and $\\rho^{(R)}_i=A^{(R)}_i/M$ the CSI is:\n",
    "\n",
    "$$\n",
    "CSI=\\sum_{i=0}^9 \\left(\\rho^{(L)}_i - \\rho^{(R)}_i\\right)\\cdot\\log\\frac{\\rho^{(L)}_i}{\\rho^{(R)}_i}\n",
    "$$\n",
    "\n",
    "Sometimes $\\rho^{(\\dots)}_{\\dots}$ is done as a percentage, which has the effect of multiplying CSI by 100. Often, one adds small number to each $\\rho^{(\\dots)}_{\\dots}$ in order to prevent dividing by zero or computing logarithms of zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf4dba-0d24-4b98-9708-e94bf2510640",
   "metadata": {},
   "source": [
    "For example, consider the temperature observations (12 months for the last century+) for [Oxford (UK)](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/oxforddata.txt) and [Durham (UK)](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/durhamdata.txt) available from the Met-Office UK (using `tmax degC` column). Breaking the the temperature into bins and collecting it into histograms:\n",
    "\n",
    "| bin number | bin lower bound (degrees) | bin upper bound (degrees) | number of Oxford observations in this bin   | number of Durham observations in this bin     |\n",
    "|------------|-----------------|---------------------------|---------------------------------------------|-----------------------------------------------|\n",
    "| 0          | low             | 6.8                       | 198                                         | 269                                           |\n",
    "| 1          | 6.8             | 8.4                       | 197                                         | 207                                           |\n",
    "| 2          | 8.4             | 9.6                       | 197                                         | 218                                           |\n",
    "| 3          | 9.6             | 11.5                      | 218                                         | 165                                           |\n",
    "| 4          | 11.5            | 13.9                      | 206                                         | 212                                           |\n",
    "| 5          | 13.9            | 16.0                      | 204                                         | 161                                           |\n",
    "| 6          | 16.0            | 18.2                      | 196                                         | 244                                           |\n",
    "| 7          | 18.2            | 19.9                      | 206                                         | 180                                           |\n",
    "| 8          | 19.9            | 21.5                      | 208                                         | 92                                            |\n",
    "| 9          | 21.5            | high                      | 205                                         | 32                                            |\n",
    "\n",
    "One can quickly estimate that CSI between these two distributions would be around 0.23. Similar approach has been described in:\n",
    "* https://parthaps77.medium.com/population-stability-index-psi-and-characteristic-stability-index-csi-in-machine-learning-6312bc52159d\n",
    "* https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf\n",
    "* https://towardsdatascience.com/psi-and-csi-top-2-model-monitoring-metrics-924a2540bed8\n",
    "* https://www.linkedin.com/pulse/credit-risk-scorecard-monitoring-tracking-shailendra/\n",
    "* https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf\n",
    "* https://towardsdatascience.com/data-drift-part-2-how-to-detect-data-drift-1f8bfa1a893e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d3246-6459-4256-9469-52b153f18f3d",
   "metadata": {},
   "source": [
    "With some simple analysis one can relate the CSI to [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). More, precisely, CSI is a symmetrized version of KL-divergence, e.g. see [SE what-is-the-intuition-behind-the-population-stability-index](https://stats.stackexchange.com/questions/219822/what-is-the-intuition-behind-the-population-stability-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cbde7-3c76-473c-9bc7-021a3a2a6d9a",
   "metadata": {},
   "source": [
    "## Treating CSI as a statistic\n",
    "\n",
    "One way to get a better handle on CSI, is to note that it is based solely on the histograms of the left and right sets of observations, i.e. the actual sets of obervations are not important. Therefore one can reduce the problem to considering the expected CSI from a set of samples drawn from the [multinomial distributions](https://en.wikipedia.org/wiki/Multinomial_distribution). There can be left and right set of probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^{(L)}&\\to {p^{(L)}_0,\\dots, p^{(L)}_9},\\quad \\sum_{i} p^{(L)}_i=1 \\\\\n",
    "p^{(R)}&\\to {p^{(R)}_0,\\dots, p^{(R)}_9},\\quad \\sum_{i} p^{(R)}_i=1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One can then draw samples with different sizes from the 'left' and 'right' multinomial distributions, using the probabilities above, and compute the appropriate CSI-s. For example, let: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^{(L)}&\\to {0.1,\\dots, 0.1} \\\\\n",
    "p^{(R)}&\\to {0.4, 0.3, 0.2, 0.1, 0.0,\\dots, 0.0} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for 10 categpories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff727105-a2ad-45be-bed0-8c6534fcffcd",
   "metadata": {},
   "source": [
    "One can obtain these using, for example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "cat_count=10 # number of categories (bins in the histogram)\n",
    "bin_bounds = np.array([-1, *range(cat_count)])+0.5 # prepare bin boundaries, e.g. [-0.5, 0.5, 1.5, ...]\n",
    "eps = 1e-3 # extra to add to force non-zero in all bins\n",
    "\n",
    "# generate the left histogram, e.g. getting \n",
    "# `left_hist = [47.001, 51.001, 38.001, 49.001, 46.001, 46.001, 49.001, 44.001, 42.001, 38.001]`\n",
    "left_count=450;\n",
    "left_hist = np.histogram(\n",
    "    npr.choice(\n",
    "        range(cat_count),\n",
    "        replace=True,\n",
    "        size=left_count,\n",
    "        p=[1./cat_count]*cat_count\n",
    "    ),\n",
    "    bins=bin_bounds\n",
    ")[0]+eps\n",
    "#\n",
    "norm_left_hist = left_hist/sum(left_hist)\n",
    "\n",
    "# generate right histogram, e.g. getting\n",
    "# `right_hist = [142.001, 112.001, 85.001, 41.001, 1e-3, ..., 1e-3])`\n",
    "right_count=380;\n",
    "right_hist = np.histogram(\n",
    "    npr.choice(\n",
    "        range(cat_count),\n",
    "        replace=True,\n",
    "        size=right_count,\n",
    "        p=[0.4, 0.3, 0.2, 0.1, *[0.0]*(cat_count-4)]\n",
    "    ),\n",
    "    bins=bin_bounds\n",
    ")[0]+eps\n",
    "norm_right_hist = right_hist/sum(right_hist)\n",
    "\n",
    "trial_csi = np.sum((norm_left_hist-norm_right_hist)*np.log(norm_left_hist/norm_right_hist))\n",
    "print(trial_csi)\n",
    "```\n",
    "\n",
    "Which ends up giving extremely large value, e.g. 4...8 depending on `eps`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32899c90-9cd8-4b92-b6fe-82bd44d80e8e",
   "metadata": {},
   "source": [
    "The actual value is not important, what matters is that:\n",
    "\n",
    "**The expectations for CSI, i.e. the null hypothesis, can be expressed in terms of the expectations for the multinomial distributions that give rise to histograms which then lead to CSI.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f8522-4fd4-4c8d-adcd-a2f6fb0a950c",
   "metadata": {},
   "source": [
    "### Null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d606a0-8da0-4902-ab72-2d73d43c4db0",
   "metadata": {},
   "source": [
    "A simple null hypothesis would be that left and right histograms come from the same multinomial distribution, perhaps with equal probabilities of each category. This however misses the noise inherent in the system under inverstigation. For example, with Met-Office temperature measurements used in preceeeding section, the temperatures in the source data are specified to within one decimal point of a degree. This would mean that 20.06 and 21.14 would be rounded up to the same value of 21.1 degrees, so one has to expect the inherent noise in the temperature be on the order of 0.04 degrees plus/minus.If the bins for the temperature are about 2 degrees wide, then the change in width of the bin due to noise can be 1.92...2.08 degrees i.e. 8% change.\n",
    "\n",
    "One way to capture this is to assume that left and right histograms come from the same multinomial distribution, but the probabilities of each category are not precise, instead they are themselves random numbers with a known distribution. A convenient parametrization here is to express probabilities of all categories as logits, and describe all logits as normally distributed random variables from a distribution with known mean and an unknown variance:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "K&:\\:number\\,of\\,categories \\\\\n",
    "s&:\\:standard\\,deviation\\,in\\,the\\,logits \\\\\n",
    "\\mu&=\\mbox{logit}\\left(1/K\\right)=-\\log\\left(K-1\\right) \\\\\n",
    "l_i & \\sim N\\left(\\mu,\\,s^2\\right),\\quad i=0,\\dots,K\\,\\quad logits\\,for\\,category\\,probabilities \\\\\n",
    "p_i &=\\sigma\\left(l_i\\right)=\\left(1+\\exp\\left(-l_i\\right)\\right)^{-1},\\quad probabilities\\,for\\,categories\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f360308-ff8d-4cfe-b466-36268fe3a104",
   "metadata": {},
   "source": [
    "Whilst this may seem somewhat complex, it is quite simple to implement\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats as sp_st\n",
    "import scipy.special as sp_spec\n",
    "\n",
    "# set up constants\n",
    "cat_count = 10 # K - number of categories (bins in the histogram)\n",
    "logit_std = 0.01 # s - standard deviation for logits\n",
    "sample_count = 400 # number of samples to draw from the multinomial distribution\n",
    "#\n",
    "logit_mu = sp_spec.logit(1./cat_count) # logit mean for all categories\n",
    "\n",
    "####\n",
    "def draw_random_histogram():\n",
    "    # draw a set of probabilities for the categories\n",
    "    logit_arr = logit_mu + npr.normal(size=cat_count) * logit_std\n",
    "    pval_arr = sp_spec.expit(logit_arr)\n",
    "    pval_arr /= sum(pval_arr) # normalize to sum to 1\n",
    "\n",
    "    # draw the samples from the categorical distribution\n",
    "    # and extract them as a histogram straight away\n",
    "    random_histogram = sp_st.multinomial.rvs(n=sample_count, p=pval_arr)\n",
    "    \n",
    "    return random_histogram\n",
    "\n",
    "####\n",
    "\n",
    "# draw left and right histograms\n",
    "left_hist = draw_random_histogram()\n",
    "right_hist = draw_random_histogram()\n",
    "\n",
    "### compute CSI\n",
    "norm_left_hist = left_hist / sum(left_hist)\n",
    "norm_right_hist = right_hist / sum(right_hist)\n",
    "csi = sum((norm_left_hist - norm_right_hist)*np.log(norm_left_hist/norm_right_hist))\n",
    "\n",
    "print('left histogram: ', left_hist)\n",
    "print('right histogram: ', right_hist)\n",
    "print(f'csi: {csi:.3e}')\n",
    "```\n",
    "\n",
    "by adjusting `logit_std` and `sample_count` one can easily see that larger number of samples does decrease the CSI, but as one increases it, the CSI usually hits a limit, which is dictated by `logit_std`. \n",
    "\n",
    "As we will demonstrate in the next section, by repeatedly sampling CSI, in a fashion shown here one can build what is essentially a hypothesis test, where CSI is treated as a statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df48ba-0d4c-4bdc-8ebd-72f1530e1c58",
   "metadata": {},
   "source": [
    "## Worked example: CSI for max monthly temperatures for different UK locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fdcad-9036-4c4f-a2a6-69ea807c7d87",
   "metadata": {},
   "source": [
    "As an example, we shall return to historical monthly temperatures provided by the Met-Office UK. Here are the locations we shall consider:\n",
    "\n",
    "* [Stornoway](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/stornowaydata.txt)\n",
    "* [Armagh](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/armaghdata.txt)\n",
    "* [Durham](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/durhamdata.txt)\n",
    "* [Bradford](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/bradforddata.txt)\n",
    "* [Sheffield](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/sheffielddata.txt)\n",
    "* [Oxford](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/oxforddata.txt)\n",
    "* [Southampton](https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/southamptondata.txt)\n",
    "\n",
    "<img src=\"uk_schematic.png\" alt=\"UK with cities of interest marked\" width=\"350\"/>\n",
    "\n",
    "Southampton, as the southern-most city will be the reference point, i.e. the grid for the histograms will be such that monthly Southampton's temperatures would be split into roghly the same-sized 10 bins.\n",
    "\n",
    "The full code for loading the data and splitting it into bins is provided in [a separate notebook notebook](extract_weather_patterns_v2.ipynb), here we shall only present illustration of the histograms extracted for Southampton.\n",
    "\n",
    "| histogram index | time-period | T < 7.6 | (7.6...9.0) | (9.0...10.2) | (10.2...12.0) | (12.0...14.25) | (14.25...16.4) | (16.4...18.4) | (18.4...20.0) |  (20.0...21.4) | T>21.4 |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| 0 | 1855-1864 | 13 | 17 | 11 |  8 | 10 | 13 | 15 | 11 |  9 | 13 |\n",
    "| 1 | 1865-1974 | 17 | 10 | 10 | 13 | 12 | 13 | 9 | 11 | 11 | 14 |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "| 12 | 1978-1987 | 12 | 13 | 12 | 13 | 8 | 16 | 9 | 17 | 9 | 11 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a590826-187d-42fa-bcbf-8045720025ef",
   "metadata": {},
   "source": [
    "So, for example, in the 10-year span 1978-1987, corresponding to histogram #12, 12 maximum monthly temperatures were below 7.6 degrees (Centigrade), 13 observations were between 7.6 and 9.0 degrees etc. Still staying with the #12 histogram, 12 out of 120 observations were below 7.6 degrees, this corresponds to proportion of 0.1 and logit of -2.197=1/(1+exp(-0.1)). Converting all observations into logits in this manner, for all locations, and then computing the mean logit and the standard deviation one finds:\n",
    "\n",
    "| location     | mean logit | logit standard deviation |\n",
    "|--------------|------------|--------------------------|\n",
    "| Stornoway    | -4.086     | 3.393                    |\n",
    "| Armagh       | -2.504     | 3.393                    |\n",
    "| Durham       | -2.503     | 1.198                    |\n",
    "| Bradford     | -2.532     | 1.166                    |\n",
    "| Sheffield    | -2.363     | 0.678                    |\n",
    "| Oxford       | -2.254     | 0.383                    |\n",
    "| Southampton  | -2.228     | 0.282                    |\n",
    "\n",
    "Unsurprisingly, Southampton's logits are closest to -2.2 and with lowest standard deviation, which corresponds to all obersvations being split roughly equally between the 10 bins. This is direct consequence on choosing bin boundaries based on Southampton as a reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f5887-0fef-4855-a517-cab618b040a9",
   "metadata": {},
   "source": [
    "Next, one can take the 10-year aggregated histograms for different locations and compare them with Southampton's by computing CSI. Using the most recent 10-year histograms in all cases one finds:\n",
    "\n",
    "| left histogram's location | right histogram's location | CSI   |\n",
    "|---------------------------|----------------------------|-------|\n",
    "| Southampton               | Stornoway                  | 3.29  |\n",
    "| Southampton               | Armagh                     | 0.36  |\n",
    "| Southampton               | Durham                     | 0.35  |\n",
    "| Southampton               | Bradford                   | 0.46  |\n",
    "| Southampton               | Sheffield                  | 0.24  |\n",
    "| Southampton               | Oxford                     | 0.08  |\n",
    "\n",
    "Which CSI is large enough to take as evidence of substantially different distribution of monthly maximum temperatures? Rule of thumb for CSI is to take anything above 0.2 as significant change, however this rule of thumb fails to take into account the noise in the data.\n",
    "\n",
    "Instead, one can adopt the null-hypothesis as left and right histograms coming from multinomial distribution where probability of each category corresponds to the sigma-function of a logit, whilst the logit is a normally distributed random variable with mean of -2.197 and standard deviation of -0.282 (comes from Southampton data). Drawing 300,000 of pairs of such histograms, with 120 samples in each (10 years of 12 months) one can find the distribution of the expected CSI (see [a separate notebook notebook](extract_weather_patterns_v2.ipynb)). Using this, one can estimate that 95% of all observed CSIs will be below 0.453. \n",
    "\n",
    "*Therefore, if one was to treat this as a hypothesis test, at 95% confidence, one would reject the null hypothesis only for Bradford and Stornoway (since there the CSI is above 0.45). Observations for all other locations do not differ sufficiently from the distribution of temperatures for Southampton.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c42006-ba7a-4993-8af6-4a21f3b7d205",
   "metadata": {},
   "source": [
    "### Specifics of implementation\n",
    "\n",
    "Drawing a large sample of CSIs, to establish thresholds, as done above, is, in principle, simple, but can be quite time-consuming if one has to cycle. A much better solution is to vectorize this process. Correspondingly a JAX-based implementation  for drawing CSI samples is offered as part of this post ([csi_calc.draw_csi_with_logit_variance](csi_calc.py)). The useage is as follows (see a separate notebook notebook)\n",
    "\n",
    "```python\n",
    "...\n",
    "import jax.config as jc\n",
    "jc.update('jax_enable_x64', True)\n",
    "import jax\n",
    "import jax.numpy as jn\n",
    "import jax.scipy as js\n",
    "import jax.random as jr\n",
    "import csi_calc as cc\n",
    "...\n",
    "\n",
    "base_logits = jn.array([1./cat_count]*cat_count)\n",
    "logit_std = agg_geos_df.query(f'location==\"{hist_choice_location}\"').logit_std.values[0]\n",
    "pop_count = chunk_size_years * 12 # number of measurements in each histogram (12 per year)\n",
    "\n",
    "csi_draw_count = 300000 # number of CSI's to compute\n",
    "\n",
    "rnd_key, csi_rnd_key = jr.split(rnd_key, num=1+1)\n",
    "tick = dt.datetime.now()\n",
    "#\n",
    "csi_samples = cc.draw_csi_with_logit_variance(\n",
    "    rnd_key=csi_rnd_key,\n",
    "    base_logits=base_logits,\n",
    "    logits_std=logit_std,\n",
    "    sample_count=csi_draw_count,\n",
    "    count_left=pop_count,\n",
    "    count_right=pop_count\n",
    ")\n",
    "#\n",
    "tock = dt.datetime.now()\n",
    "time_delta = tock-tick\n",
    "time_delta_sec = time_delta.seconds + time_delta.microseconds * 1e-6\n",
    "\n",
    "highq = 0.95\n",
    "highq_csi = jn.quantile(csi_samples, q=highq)\n",
    "\n",
    "print(f'Time taken: {time_delta_sec:.3f} sec for {csi_draw_count} samples.')\n",
    "print(f'Quantile q={highq:.3f} corresponds to CSI={highq_csi:.3e}')\n",
    "```\n",
    "\n",
    "The calculation takes 10-20 sec on an average laptop, and can effectively use available cores and RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5506b6-4f41-4e43-a0b8-9bed9d7458e1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Characteristic Stability Index can be a useful way for estimating the drift in the distribution of a random variable, given a test set and a reference set. To draw informative conclusions it is best avoid using rule-of-thumb thresholds, as these ignore:\n",
    "\n",
    "* Size of your dataset\n",
    "* Inherent noise in your dataset\n",
    "\n",
    "Instead, one can estimate appropriate thresholds by building a sound Null Hypothesis and then estimating the expected distribution of CSI under this Null Hypothesis. Modern tools such as JAX make this a relatively inexpensive and scaleable exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa920c3c-12e8-4242-ade9-ec190cfee670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
